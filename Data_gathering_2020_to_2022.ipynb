{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "J9k1FNXaOG0N",
   "metadata": {
    "id": "J9k1FNXaOG0N"
   },
   "source": [
    "## **Data Gathering for JOB DISSATISFACTION IN THE UK USING REDDIT Archive for (2020-2022 Dataset)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YHwHpMh7OfLt",
   "metadata": {
    "id": "YHwHpMh7OfLt"
   },
   "source": [
    "#### **Import the necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8dcf505-d640-4f24-bddc-d482af22e6e6",
   "metadata": {
    "id": "a8dcf505-d640-4f24-bddc-d482af22e6e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import zstandard as zstd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"zstandard\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VkuQ3b7DO1V5",
   "metadata": {
    "id": "VkuQ3b7DO1V5"
   },
   "source": [
    "#### **Data Source**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bxZhtWJPLm_",
   "metadata": {
    "id": "3bxZhtWJPLm_"
   },
   "source": [
    "The data was gathered from the **Reddit archive** available at [Eye.eu](http://eye.eu/).  \n",
    "The files are provided in **`.zst`** format, which contains compressed Reddit data for subreddit.\n",
    "\n",
    "A total of **10 subreddits** were downloaded, including both **submissions (posts)** and **comments**, to ensure a comprehensive view of discussions related to job dissatisfaction.  \n",
    "\n",
    "The subreddits include:\n",
    "\n",
    "- `UKJobs`  \n",
    "- `AskUK`  \n",
    "- `CasualUK`  \n",
    "- `unitedkingdom`  \n",
    "- `antiwork`  \n",
    "- `WorkReform`  \n",
    "- `careerguidance`  \n",
    "- `AskHR`  \n",
    "- `britishproblems`  \n",
    "- `recruitinghell`  \n",
    "- `WorkReformUK`  \n",
    "\n",
    "Data from these subreddits was Collected and we will later filter it for the years **2020 to 2022**.  \n",
    "A keyword-based filtering process was then applied to extract posts and comments specifically related to **job dissatisfaction** and **workplace sentiment**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DixnnNYJQmXx",
   "metadata": {
    "id": "DixnnNYJQmXx"
   },
   "source": [
    "###### To find posts and comments that talk about **job dissatisfaction**, we created a list of keywords that people often use when expressing frustration about their jobs. These keywords helped us filter through all the Reddit data collected from **2020 to 2022** and focus only on discussions that mention things like **burnout**, **bad management**, **low pay**, or simply **hating their job**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b0d016c-191d-4844-922a-0de518941dcb",
   "metadata": {
    "id": "7b0d016c-191d-4844-922a-0de518941dcb",
    "outputId": "cd66b85f-1ea3-41f7-8009-478aa426b02f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['job dissatisfaction', 'hate my job', 'toxic workplace', 'burnout', 'overworked', 'bad boss', 'micromanagement', 'low pay', 'underpaid', 'quit my job', 'resign', 'stress at work', 'bullying at work', 'zero hours', 'unhappy at work', 'stressful job', 'poor management', 'burnt out', 'miserable at work', 'dead-end job', 'exploited at work', 'no work-life balance', 'hate going to work', 'rejection', 'really bad', 'unreasonable', 'management cuts benefits']\n"
     ]
    }
   ],
   "source": [
    "DISSATISFACTION_TERMS = [\n",
    "    \"job dissatisfaction\", \"hate my job\", \"toxic workplace\", \"burnout\", \"overworked\",\n",
    "    \"bad boss\", \"micromanagement\", \"low pay\", \"underpaid\", \"quit my job\", \"resign\",\n",
    "    \"stress at work\", \"bullying at work\", \"zero hours\", \"unhappy at work\",\n",
    "    \"stressful job\", \"poor management\", \"burnt out\", \"miserable at work\",\n",
    "    \"dead-end job\", \"exploited at work\", \"no work-life balance\",\n",
    "    \"hate going to work\", \"rejection\", \"really bad\", \"unreasonable\",\n",
    "    \"management cuts benefits\"\n",
    "]\n",
    "\n",
    "DISSATISFACTION_TERMS = [t.lower() for t in DISSATISFACTION_TERMS]\n",
    "print(DISSATISFACTION_TERMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jhHM4JPPRFUR",
   "metadata": {
    "id": "jhHM4JPPRFUR"
   },
   "source": [
    "##### Just like with job dissatisfaction, we also wanted to find posts and comments where people talk positively about their work. To do this, we created a list of keywords that capture feelings of **job satisfaction**, such as enjoying work, having a good boss, fair pay, or a supportive team. These keywords were used to filter Reddit data (2020–2022) for content that reflects **positive work experiences** and **employee satisfaction**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b4c856-22c9-4d09-8ef8-47bb7bb14cff",
   "metadata": {
    "id": "06b4c856-22c9-4d09-8ef8-47bb7bb14cff",
    "outputId": "d54fd65e-64ce-414b-e782-44adda8dc42a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love my job', 'happy at work', 'good boss', 'great team', 'work life balance', 'supportive manager', 'flexible working', 'fair pay']\n"
     ]
    }
   ],
   "source": [
    "SATISFACTION_TERMS = [\n",
    "    \"love my job\", \"happy at work\", \"good boss\", \"great team\", \"work life balance\",\n",
    "    \"supportive manager\", \"flexible working\", \"fair pay\"\n",
    "]\n",
    "\n",
    "SATISFACTION_TERMS = [t.lower() for t in SATISFACTION_TERMS]\n",
    "print(SATISFACTION_TERMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VfBgIO7pRUhg",
   "metadata": {
    "id": "VfBgIO7pRUhg"
   },
   "source": [
    "#### **Extracting the Year from Unix Timestamps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65paQz6ZRXTo",
   "metadata": {
    "id": "65paQz6ZRXTo"
   },
   "source": [
    "##### Each Reddit post and comment in the dataset includes a **Unix timestamp** (the number of seconds since January 1, 1970). To make the data easier to analyze by year, we created a small helper function that converts each timestamp into its corresponding **UTC year**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d90558c-4f57-46a7-a6dd-aca45f0f0a80",
   "metadata": {
    "id": "5d90558c-4f57-46a7-a6dd-aca45f0f0a80"
   },
   "outputs": [],
   "source": [
    "def get_year_from_utc(ts):\n",
    "    \"\"\"Convert Unix timestamp to a year (UTC).\"\"\"\n",
    "    return datetime.utcfromtimestamp(int(ts)).year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TmsQnOJIRoC4",
   "metadata": {
    "id": "TmsQnOJIRoC4"
   },
   "source": [
    "#### **Sentiment Classification Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ID6ncUvmR2Gq",
   "metadata": {
    "id": "ID6ncUvmR2Gq"
   },
   "source": [
    "##### To identify whether a Reddit post or comment expresses **job satisfaction**, **job dissatisfaction**, or **neutral** sentiment, we created a simple keyword-based function called `classify_sentiment()`. This function checks each text entry against predefined keyword lists:\n",
    "- `DISSATISFACTION_TERMS` = Negative experiences (e.g., “hate my job”, “toxic workplace”)  \n",
    "- `SATISFACTION_TERMS` = Positive experiences (e.g., “love my job”, “great team”)  \n",
    "\n",
    "##### If none of the keywords are found, the function returns `\"none\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a35af612-9e77-4ad9-b361-8f205454903c",
   "metadata": {
    "id": "a35af612-9e77-4ad9-b361-8f205454903c"
   },
   "outputs": [],
   "source": [
    "def classify_sentiment(text):\n",
    "    if text is None:\n",
    "        text_l = \"\"\n",
    "    else:\n",
    "        text_l = text.lower()\n",
    "\n",
    "    # dissatisfaction first\n",
    "    for term in DISSATISFACTION_TERMS:\n",
    "        if term in text_l:\n",
    "            return \"dissatisfaction\"\n",
    "\n",
    "    # satisfaction second\n",
    "    for term in SATISFACTION_TERMS:\n",
    "        if term in text_l:\n",
    "            return \"satisfaction\"\n",
    "\n",
    "    return \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5AobUHuPSF5p",
   "metadata": {
    "id": "5AobUHuPSF5p"
   },
   "source": [
    "#### **Defining Years of Interest**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u6o5XYLQSJju",
   "metadata": {
    "id": "u6o5XYLQSJju"
   },
   "source": [
    "##### Since the Reddit data was collected between **2020 and 2022**, we created a simple structure to help organize the posts and comments by year. This makes it easier to process, analyze, or export data for each year separately (for example, 2020 data → one file, 2021 data → another)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccb5db15-cb82-4273-a132-02bb357cf8b0",
   "metadata": {
    "id": "ccb5db15-cb82-4273-a132-02bb357cf8b0",
    "outputId": "96f890e0-5327-4ca0-a39a-6d92f29c3c72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2020: [], 2021: [], 2022: []}\n"
     ]
    }
   ],
   "source": [
    "YEARS_OF_INTEREST = [2020, 2021, 2022]\n",
    "\n",
    "year_buckets = {year: [] for year in YEARS_OF_INTEREST}\n",
    "print(year_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qvwg_NVYSb-I",
   "metadata": {
    "id": "qvwg_NVYSb-I"
   },
   "source": [
    "#### **Filtering UK-Related Content**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LAyHSJ0UShLJ",
   "metadata": {
    "id": "LAyHSJ0UShLJ"
   },
   "source": [
    "##### Because this project focuses on **UK-based job discussions**, we needed a way to identify whether each Reddit post or comment was **UK-related**.  \n",
    "To do that, we created a small helper function called `is_uk_related()`.\n",
    "\n",
    "This function checks:\n",
    "1. If the post comes from a **UK-focused subreddit** (like `r/UKJobs` or `r/AskUK`), **or**  \n",
    "2. If the post text mentions **UK regions or cities** (like “London”, “Scotland”, “Manchester”, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89d79387-e776-409c-8f97-6bc5e25f6718",
   "metadata": {
    "id": "89d79387-e776-409c-8f97-6bc5e25f6718"
   },
   "outputs": [],
   "source": [
    "UK_REGEX = re.compile(\n",
    "    r\"\\b(UK|United Kingdom|England|Scotland|Wales|Northern Ireland|London|Manchester|Birmingham|Leeds|Glasgow|Bristol|Liverpool)\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def is_uk_related(text: str, subreddit: str) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if the post is considered UK-related.\n",
    "\n",
    "    We treat a post as UK-related if:\n",
    "    - it's from a known UK subreddit, OR\n",
    "    - the text contains UK place/region keywords\n",
    "    \"\"\"\n",
    "    if subreddit.lower() in {\"ukjobs\", \"askuk\", \"casualuk\", \"unitedkingdom\", \"britishproblems\"}:\n",
    "        return True\n",
    "    return bool(UK_REGEX.search(text or \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S5I64-FRS2ww",
   "metadata": {
    "id": "S5I64-FRS2ww"
   },
   "source": [
    "#### **Processing Reddit Submission Files (`.zst`)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ncvWvm6ZS5vB",
   "metadata": {
    "id": "ncvWvm6ZS5vB"
   },
   "source": [
    "To build the final dataset, we process each compressed Reddit file (`.zst`) and extract only the posts that match our criteria:\n",
    "- Within the years **2020–2022**\n",
    "- Related to the **UK**\n",
    "- Expressing **job satisfaction** or **job dissatisfaction**\n",
    "\n",
    "The function below reads a `.zst` file **line by line** (in a memory-efficient way), filters the data, and stores the results into `year_buckets`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efa6c352-60cc-4f4d-a196-a01aa293d168",
   "metadata": {
    "id": "efa6c352-60cc-4f4d-a196-a01aa293d168"
   },
   "outputs": [],
   "source": [
    "def process_submission_file(zst_path, year_buckets):\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "\n",
    "    with open(zst_path, \"rb\") as compressed_file:\n",
    "        with dctx.stream_reader(compressed_file) as reader:\n",
    "            buffer = b\"\"\n",
    "\n",
    "            while True:\n",
    "                chunk = reader.read(2**20)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                buffer += chunk\n",
    "\n",
    "                *lines, buffer = buffer.split(b\"\\n\")\n",
    "\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "                    created_utc = obj.get(\"created_utc\")\n",
    "                    if created_utc is None:\n",
    "                        continue\n",
    "\n",
    "                    year = get_year_from_utc(created_utc)\n",
    "                    if year not in YEARS_OF_INTEREST:\n",
    "                        continue\n",
    "\n",
    "                    data_id = obj.get(\"id\", \"\")\n",
    "                    subreddit = obj.get(\"subreddit\", \"\")\n",
    "                    selftext = obj.get(\"selftext\", \"\") or obj.get(\"body\", \"\")\n",
    "\n",
    "                    full_text = f\"{selftext}\".strip()\n",
    "\n",
    "                    if not is_uk_related(full_text, subreddit):\n",
    "                        continue\n",
    "\n",
    "                    sent = classify_sentiment(full_text)\n",
    "                    if sent == \"none\":\n",
    "                        continue\n",
    "\n",
    "                    row = {\n",
    "                        \"id\": data_id,\n",
    "                        \"created_utc\": int(created_utc),\n",
    "                        \"year\": year,\n",
    "                        \"subreddit\": subreddit,\n",
    "                        \"sentiment\": sent,\n",
    "                    }\n",
    "\n",
    "                    year_buckets[year].append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2pUvX0hTTF2I",
   "metadata": {
    "id": "2pUvX0hTTF2I"
   },
   "source": [
    "##### **Running the Processing Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EFArV1a3TMhg",
   "metadata": {
    "id": "EFArV1a3TMhg"
   },
   "source": [
    "After defining the helper functions for filtering and classification,  \n",
    "we use `run_process()` to **automate the processing** of all `.zst` files in the project directory.\n",
    "\n",
    "This function finds every compressed Reddit file that matches a given `type` (for example, `\"submission\"` or `\"comment\"`)  \n",
    "and processes them one by one using the `process_submission_file()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e123d5d8-1d1d-464a-b139-ef44548d08b4",
   "metadata": {
    "id": "e123d5d8-1d1d-464a-b139-ef44548d08b4"
   },
   "outputs": [],
   "source": [
    "def run_process(types):\n",
    "    cwd = os.getcwd()\n",
    "    directory = cwd\n",
    "    file_pattern = f\"*{types}*.zst*\"\n",
    "\n",
    "    ZST_FILES = glob.glob(os.path.join(directory, file_pattern))\n",
    "\n",
    "    for path in ZST_FILES:\n",
    "        print(\"Processing:\", path)\n",
    "        process_submission_file(path, year_buckets)\n",
    "    print(\"Done streaming all files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Al2yAhggTaAx",
   "metadata": {
    "id": "Al2yAhggTaAx"
   },
   "source": [
    "#### **Running the Data Processing Pipeline for Post**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NqcRGlHbThFi",
   "metadata": {
    "id": "NqcRGlHbThFi"
   },
   "source": [
    "After setting up all helper functions and defining the keyword filters,  \n",
    "the next step is to actually **run the pipeline** to process all `.zst` Reddit files.\n",
    "\n",
    "We start by processing all **submission** files (Reddit posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb9701f8-7178-410e-92b1-f8a9ba55f7bb",
   "metadata": {
    "id": "eb9701f8-7178-410e-92b1-f8a9ba55f7bb",
    "outputId": "7579f696-6e18-44ef-b2df-f04fdfe6609f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\antiwork_submissions.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\923826\\AppData\\Local\\Temp\\ipykernel_13088\\3020403082.py:3: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  return datetime.utcfromtimestamp(int(ts)).year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\AskHR_submissions.zst\n",
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\AskUK_submissions (1).zst\n",
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\britishproblems_submissions.zst\n",
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\careerguidance_submissions.zst\n",
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\CasualUK_submissions.zst\n",
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\recruitinghell_submissions.zst\n",
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\UKJobs_submissions (1).zst\n",
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\unitedkingdom_submissions.zst\n",
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\WorkReform_submissions.zst\n",
      "Done streaming all files.\n"
     ]
    }
   ],
   "source": [
    "run_process(\"submissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0B9Cw9TzTn8Y",
   "metadata": {
    "id": "0B9Cw9TzTn8Y"
   },
   "source": [
    "#### **Checking Collected Data by Year for Submission (Post)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Eorw7nFTTzYQ",
   "metadata": {
    "id": "Eorw7nFTTzYQ"
   },
   "source": [
    "After running the data processing pipeline, it’s important to verify how many posts and comments were collected for each year (2020–2022). This quick check helps confirm that the filtering and extraction process worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0943c503-a43f-4fd8-9555-2a32521766cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "0943c503-a43f-4fd8-9555-2a32521766cf",
    "outputId": "8c5631d4-d378-4674-ad00-1fecaad47f66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 items collected: 317\n",
      "2021 items collected: 589\n",
      "2022 items collected: 996\n"
     ]
    }
   ],
   "source": [
    "for y in YEARS_OF_INTEREST:\n",
    "    print(y, \"items collected:\", len(year_buckets[y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mz3SVHasT9Uh",
   "metadata": {
    "id": "mz3SVHasT9Uh"
   },
   "source": [
    "#### **Saving Processed Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WTh7Q1iqUDC4",
   "metadata": {
    "id": "WTh7Q1iqUDC4"
   },
   "source": [
    "Once all Reddit data has been streamed, filtered, and organized into yearly buckets,  \n",
    "the final step is to **save each year’s dataset** as a CSV file for further analysis.\n",
    "\n",
    "The function `save_result()` does exactly that — it loops through the collected data for each year,  \n",
    "creates a DataFrame, and saves it into the `result` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a094fe4-1002-457a-bbd5-197439c98a94",
   "metadata": {
    "id": "2a094fe4-1002-457a-bbd5-197439c98a94"
   },
   "outputs": [],
   "source": [
    "def save_result(filetype):\n",
    "    output_folder = r\"result\"  # make sure this folder exists\n",
    "\n",
    "    for year in YEARS_OF_INTEREST:\n",
    "        rows = year_buckets[year]\n",
    "        if len(rows) == 0:\n",
    "            print(f\"Year {year}: no matches, skipping CSV.\")\n",
    "            continue\n",
    "        df = pd.DataFrame(rows)\n",
    "\n",
    "        out_path = fr\"{output_folder}\\subreddit_{year}_{filetype}.csv\"\n",
    "        df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "        print(f\"Saved {len(df)} rows for {year} -> {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ESP1pkRwUFvJ",
   "metadata": {
    "id": "ESP1pkRwUFvJ"
   },
   "source": [
    "##### **Saving for post using the function that was created**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aa6347e-b6db-4b54-8a67-a918f268f358",
   "metadata": {
    "id": "8aa6347e-b6db-4b54-8a67-a918f268f358",
    "outputId": "d50c7b23-afa3-401a-f214-138e9d40df32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 317 rows for 2020 -> result\\subreddit_2020_post.csv\n",
      "Saved 589 rows for 2021 -> result\\subreddit_2021_post.csv\n",
      "Saved 996 rows for 2022 -> result\\subreddit_2022_post.csv\n"
     ]
    }
   ],
   "source": [
    "save_result(\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uZcnvZdUUXvx",
   "metadata": {
    "id": "uZcnvZdUUXvx"
   },
   "source": [
    "#### **Running the Data Processing Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ff58f7e-b9b2-4265-af13-7d185ff7bd28",
   "metadata": {
    "id": "5ff58f7e-b9b2-4265-af13-7d185ff7bd28",
    "outputId": "bdcc9a55-0eec-4922-a8c7-eaced914c160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\antiwork_comments (1).zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\923826\\AppData\\Local\\Temp\\ipykernel_13088\\3020403082.py:3: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  return datetime.utcfromtimestamp(int(ts)).year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\AskHR_comments.zst\n",
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\AskUK_comments.zst\n",
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\britishproblems_comments (1).zst\n",
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\britishproblems_comments.zst\n",
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\careerguidance_comments.zst\n",
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\CasualUK_comments.zst\n",
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\recruitinghell_comments.zst\n",
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\UKJobs_comments (2).zst\n",
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\unitedkingdom_comments.zst\n",
      "Processing: C:\\users\\923826\\OneDrive - hull.ac.uk\\Desktop\\dammy\\Dissertation\\WorkReform_comments.zst\n",
      "Done streaming all files.\n"
     ]
    }
   ],
   "source": [
    "run_process(\"comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i_XQ9G7DUiWh",
   "metadata": {
    "id": "i_XQ9G7DUiWh"
   },
   "source": [
    "#### **Checking Collected Data by Year for Submission (Post)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bf7a6d0-b629-4cae-85a5-b495b591bfaa",
   "metadata": {
    "id": "5bf7a6d0-b629-4cae-85a5-b495b591bfaa",
    "outputId": "e0c1d152-5200-4082-e74f-66a368a5090a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 items collected: 14356\n",
      "2021 items collected: 23216\n",
      "2022 items collected: 38062\n"
     ]
    }
   ],
   "source": [
    "for y in YEARS_OF_INTEREST:\n",
    "    print(y, \"items collected:\", len(year_buckets[y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qvX96TfFUntI",
   "metadata": {
    "id": "qvX96TfFUntI"
   },
   "source": [
    "#### **Saving for post using the function that was created**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d0f0672-145b-4bd8-afa5-f1c26f880a09",
   "metadata": {
    "id": "8d0f0672-145b-4bd8-afa5-f1c26f880a09",
    "outputId": "f6f747e2-6495-4a5c-a54f-f2d9e810758e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 14356 rows for 2020 -> result\\subreddit_2020_comment.csv\n",
      "Saved 23216 rows for 2021 -> result\\subreddit_2021_comment.csv\n",
      "Saved 38062 rows for 2022 -> result\\subreddit_2022_comment.csv\n"
     ]
    }
   ],
   "source": [
    "save_result(\"comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c4125-50f9-4ca1-af8d-d1c8f91aacb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
